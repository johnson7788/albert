ALBERT
======

***************New March 28, 2020 ***************

Add a colab [tutorial](https://github.com/google-research/albert/blob/master/albert_glue_fine_tuning_tutorial.ipynb) to run fine-tuning for GLUE datasets.

***************New January 7, 2020 ***************

v2 TF-Hubæ¨¡å‹ç°åœ¨åº”è¯¥å¯ä»¥åœ¨TF 1.15ä¸Šä½¿ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ä»å›¾ä¸­åˆ é™¤äº†å±€éƒ¨Einsum opã€‚ è¯·å‚é˜…ä¸‹é¢çš„æ›´æ–°çš„TF-Hubé“¾æ¥ã€‚


***************New December 30, 2019 ***************

ä¸­æ–‡æ¨¡å‹å‘å¸ƒã€‚ æ„Ÿè°¢[CLUE team ](https://github.com/CLUEbenchmark/CLUE)æä¾›çš„è®­ç»ƒæ•°æ®ã€‚æ˜¯v1ç‰ˆæœ¬çš„, å¯ä»¥ç›´æ¥wgetä¸‹è½½ï¼ŒğŸ‘!

- [Base](https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz)
- [Large](https://storage.googleapis.com/albert_models/albert_large_zh.tar.gz)
- [Xlarge](https://storage.googleapis.com/albert_models/albert_xlarge_zh.tar.gz)
- [Xxlarge](https://storage.googleapis.com/albert_models/albert_xxlarge_zh.tar.gz)

ALBERTæ¨¡å‹çš„ç‰ˆæœ¬2å·²å‘å¸ƒã€‚ 

- Base: [[Tar file](https://storage.googleapis.com/albert_models/albert_base_v2.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_base/3)]
- Large: [[Tar file](https://storage.googleapis.com/albert_models/albert_large_v2.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_large/3)]
- Xlarge: [[Tar file](https://storage.googleapis.com/albert_models/albert_xlarge_v2.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_xlarge/3)]
- Xxlarge: [[Tar file](https://storage.googleapis.com/albert_models/albert_xxlarge_v2.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_xxlarge/3)]

åœ¨æ­¤ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ¨¡å‹åº”ç”¨â€œæ— dropoutâ€ï¼Œâ€œå…¶ä»–è®­ç»ƒæ•°æ®â€å’Œâ€œé•¿æ—¶é—´è®­ç»ƒâ€ç­–ç•¥ã€‚ æˆ‘ä»¬è®­ç»ƒåŸºäºALBERTçš„10Mæ­¥ï¼Œè®­ç»ƒå…¶ä»–æ¨¡å‹3Mçš„æ­¥ã€‚

ä¸v1æ¨¡å‹çš„ç»“æœæ¯”è¾ƒå¦‚ä¸‹ï¼š 


|                | Average  | SQuAD1.1 | SQuAD2.0 | MNLI     | SST-2    | RACE     |
|----------------|----------|----------|----------|----------|----------|----------|
|V2              |
|ALBERT-base     |82.3      |90.2/83.2 |82.1/79.3 |84.6      |92.9      |66.8      |
|ALBERT-large    |85.7      |91.8/85.2 |84.9/81.8 |86.5      |94.9      |75.2      |
|ALBERT-xlarge   |87.9      |92.9/86.4 |87.9/84.1 |87.9      |95.4      |80.7      |
|ALBERT-xxlarge  |90.9      |94.6/89.1 |89.8/86.9 |90.6      |96.8      |86.8      |
|V1              |
|ALBERT-base     |80.1      |89.3/82.3 | 80.0/77.1|81.6      |90.3      | 64.0     |
|ALBERT-large    |82.4      |90.6/83.9 | 82.3/79.4|83.5      |91.7      | 68.5     |
|ALBERT-xlarge   |85.5      |92.5/86.1 | 86.1/83.1|86.4      |92.4      | 74.8     |
|ALBERT-xxlarge  |91.0      |94.8/89.3 | 90.2/87.4|90.8      |96.9      | 86.5     |

æ¯”è¾ƒè¡¨æ˜ï¼Œå¯¹äºåŸºäºALBERTçš„ï¼ŒALBERT-largeçš„å’ŒALBERT-xlargeçš„ï¼Œv2ä¼˜äºv1ï¼Œè¡¨æ˜åº”ç”¨ä»¥ä¸Šä¸‰ç§ç­–ç•¥çš„é‡è¦æ€§ã€‚ 
å¹³å‡è€Œè¨€ï¼Œç”±äºä»¥ä¸‹ä¸¤ä¸ªåŸå› ï¼ŒALBERT-xxlargeæ¯”v1ç¨å·®ï¼š
1)è®­ç»ƒé¢å¤–çš„1.5Mæ­¥(è¿™ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«æ˜¯è®­ç»ƒ1.5Mæ­¥å’Œ3Mæ­¥)å¹¶æ²¡æœ‰å¸¦æ¥æ˜æ˜¾çš„æ€§èƒ½æ”¹è¿›ã€‚ 
2)å¯¹äºv1ï¼Œæˆ‘ä»¬åœ¨BERTï¼ŒRobertaå’ŒXLnetç»™å®šçš„å‚æ•°é›†ä¸­è¿›è¡Œäº†ä¸€äº›è¶…å‚æ•°æœç´¢ã€‚ 
å¯¹äºv2ï¼Œæˆ‘ä»¬ä»…é‡‡ç”¨v1ä¸­çš„å‚æ•°ï¼Œé™¤äº†RACEï¼Œæˆ‘ä»¬ä½¿ç”¨çš„å­¦ä¹ ç‡æ˜¯1e-5å’Œ0 [ALBERT DR](https://arxiv.org/pdf/1909.11942.pdf) (ALBERTçš„dropout rate å¾®è°ƒ)ã€‚
åŸå§‹(v1)RACEè¶…å‚æ•°å°†å¯¼è‡´v2æ¨¡å‹çš„æ¨¡å‹åˆ†æ­§ã€‚ é‰´äºä¸‹æ¸¸ä»»åŠ¡å¯¹å¾®è°ƒè¶…å‚æ•°æ•æ„Ÿï¼Œå› æ­¤æˆ‘ä»¬åº”è°¨æ…å¯¹å¾…æ‰€è°“çš„ç»†å¾®æ”¹è¿›ã€‚ 

ALBERTæ˜¯BERTçš„â€œç²¾ç®€ç‰ˆâ€ç‰ˆæœ¬ï¼Œå®ƒæ˜¯ä¸€ç§æµè¡Œçš„æ— ç›‘ç£è¯­è¨€è¡¨ç¤ºå­¦ä¹ ç®—æ³•ã€‚ 
ALBERTä½¿ç”¨å‚æ•°ç¼©å‡æŠ€æœ¯ï¼Œå¯è¿›è¡Œå¤§è§„æ¨¡é…ç½®ï¼Œå…‹æœå…ˆå‰çš„å†…å­˜é™åˆ¶å¹¶åœ¨æ¨¡å‹é™çº§æ–¹é¢å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚ 


æœ‰å…³è¯¥ç®—æ³•çš„æŠ€æœ¯è¯´æ˜ï¼Œè¯·å‚è§æˆ‘ä»¬çš„è®ºæ–‡ï¼š 

[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut

Release Notes
=============

- Initial release: 10/9/2019

Results
=======

ALBERTåœ¨GLUEåŸºå‡†æµ‹è¯•ç»“æœä¸Šä½¿ç”¨å•æ¨¡å‹è®¾ç½®çš„æ€§èƒ½ 
dev:

| Models            | MNLI     | QNLI     | QQP      | RTE      | SST      | MRPC     | CoLA     | STS      |
|-------------------|----------|----------|----------|----------|----------|----------|----------|----------|
| BERT-large        | 86.6     | 92.3     | 91.3     | 70.4     | 93.2     | 88.0     | 60.6     | 90.0     |
| XLNet-large       | 89.8     | 93.9     | 91.8     | 83.8     | 95.6     | 89.2     | 63.6     | 91.8     |
| RoBERTa-large     | 90.2     | 94.7     | **92.2** | 86.6     | 96.4     | **90.9** | 68.0     | 92.4     |
| ALBERT (1M)       | 90.4     | 95.2     | 92.0     | 88.1     | 96.8     | 90.2     | 68.7     | 92.7     |
| ALBERT (1.5M)     | **90.8** | **95.3** | **92.2** | **89.2** | **96.9** | **90.9** | **71.4** | **93.0** |

ä½¿ç”¨å•æ¨¡å‹çš„ALBERT-xxlåœ¨SQuaDå’ŒRACEåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ 
setup:

|Models                    | SQuAD1.1 dev  | SQuAD2.0 dev  | SQuAD2.0 test | RACE test (Middle/High) |
|--------------------------|---------------|---------------|---------------|-------------------------|
|BERT-large                | 90.9/84.1     | 81.8/79.0     | 89.1/86.3     | 72.0 (76.6/70.1)        |
|XLNet                     | 94.5/89.0     | 88.8/86.1     | 89.1/86.3     | 81.8 (85.5/80.2)        |
|RoBERTa                   | 94.6/88.9     | 89.4/86.5     | 89.8/86.8     | 83.2 (86.5/81.3)        |
|UPM                       | -             | -             | 89.9/87.2     | -                       |
|XLNet + SG-Net Verifier++ | -             | -             | 90.1/87.2     | -                       |
|ALBERT (1M)               | 94.8/89.2     | 89.9/87.2     | -             | 86.0 (88.2/85.1)        |
|ALBERT (1.5M)             | **94.8/89.3** | **90.2/87.4** | **90.9/88.1** | **86.5 (89.0/85.5)**    |

# ç›®å½•ç»“æ„
albert
```buildoutcfg
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ albert_glue_fine_tuning_tutorial.ipynb      glueçš„colabå¾®è°ƒè„šæœ¬
â”œâ”€â”€ classifier_utils.py          åˆ†ç±»çš„utilså‡½æ•°
â”œâ”€â”€ create_pretraining_data.py   åˆ›å»ºé¢„è®­ç»ƒæ•°æ®
â”œâ”€â”€ export_checkpoints.py      å¯¼å‡ºcheckpoints
â”œâ”€â”€ export_to_tfhub.py          å¯¼å‡ºåˆ°tfhub
â”œâ”€â”€ fine_tuning_utils.py          å¾®è°ƒçš„utils
â”œâ”€â”€ lamb_optimizer.py           lambä¼˜åŒ–å™¨
â”œâ”€â”€ modeling.py                 albertæ¨¡å‹
â”œâ”€â”€ modeling_test.py             albertæ¨¡å‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ optimization.py              ä¼˜åŒ–å™¨
â”œâ”€â”€ optimization_test.py
â”œâ”€â”€ race_utils.py               raceæ•°æ®é›†utils
â”œâ”€â”€ requirements.txt            ä¾èµ–åŒ…
â”œâ”€â”€ run_classifier.py           é€šç”¨åˆ†ç±»è„šæœ¬
â”œâ”€â”€ run_glue.sh
â”œâ”€â”€ run_pretraining.py          è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ run_pretraining_test.py
â”œâ”€â”€ run_race.py
â”œâ”€â”€ run_squad_v1.py
â”œâ”€â”€ run_squad_v2.py
â”œâ”€â”€ run_trivial_model_test.sh
â”œâ”€â”€ squad_utils.py
â”œâ”€â”€ tokenization.py               tokenizer
â”œâ”€â”€ tokenization_test.py

```

# ä¾èµ–åŒ…
```buildoutcfg
python 3.7
pip install -r requirement.txt
```

Pre-trained Models
==================
TF-Hub modules are available:

- Base: [[Tar file](https://storage.googleapis.com/albert_models/albert_base_v1.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_base/1)]
- Large: [[Tar file](https://storage.googleapis.com/albert_models/albert_large_v1.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_large/1)]
- Xlarge: [[Tar file](https://storage.googleapis.com/albert_models/albert_xlarge_v1.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_xlarge/1)]
- Xxlarge: [[Tar file](https://storage.googleapis.com/albert_models/albert_xxlarge_v1.tar.gz)] [[TF-Hub](https://tfhub.dev/google/albert_xxlarge/1)]

Example usage of the TF-Hub module in code:

```
tags = set()
if is_training:
  tags.add("train")
albert_module = hub.Module("https://tfhub.dev/google/albert_base/1", tags=tags,
                           trainable=True)
albert_inputs = dict(
    input_ids=input_ids,
    input_mask=input_mask,
    segment_ids=segment_ids)
albert_outputs = albert_module(
    inputs=albert_inputs,
    signature="tokens",
    as_dict=True)

# If you want to use the token-level output, use
# albert_outputs["sequence_output"] instead.
output_layer = albert_outputs["pooled_output"]
```

è¯¥repositoryä¸­çš„å¤§å¤šæ•°å¾®è°ƒè„šæœ¬éƒ½é€šè¿‡--albert_hub_module_handleæ ‡å¿—æ¥æ”¯æŒTF-hubæ¨¡å—ã€‚ 

é¢„è®­ç»ƒè®¾ç½®
=========================
è¦é¢„è®­ç»ƒALBERTï¼Œè¯·ä½¿ç”¨`run_pretraining.py`ï¼š 

```
pip install -r albert/requirements.txt
python -m albert.run_pretraining \
    --input_file=... \
    --output_dir=... \
    --init_checkpoint=... \
    --albert_config_file=... \
    --do_train \
    --do_eval \
    --train_batch_size=4096 \
    --eval_batch_size=64 \
    --max_seq_length=512 \
    --max_predictions_per_seq=20 \
    --optimizer='lamb' \
    --learning_rate=.00176 \
    --num_train_steps=125000 \
    --num_warmup_steps=3125 \
    --save_checkpoints_steps=5000
```

Fine-tuning on GLUE
===================
è¦åœ¨GLUEä¸Šå¾®è°ƒå’Œè¯„ä¼°ç»è¿‡é¢„è®­ç»ƒçš„ALBERTï¼Œè¯·å‚è§ä¾¿æ·è„šæœ¬`run_glue.sh`ã€‚

Lower-levelç”¨ä¾‹å¯èƒ½è¦ç›´æ¥ä½¿ç”¨`run_classifier.py`è„šæœ¬ã€‚
`run_classifier.py`è„šæœ¬ç”¨äºå¾®è°ƒå’Œè¯„ä¼°å•ä¸ªGLUEåŸºå‡†æµ‹è¯•ä»»åŠ¡(ä¾‹å¦‚MNLI)ä¸Šçš„ALBERTï¼š 

```
pip install -r albert/requirements.txt
python -m albert.run_classifier \
  --data_dir=... \
  --output_dir=... \
  --init_checkpoint=... \
  --albert_config_file=... \
  --spm_model_file=... \
  --do_train \
  --do_eval \
  --do_predict \
  --do_lower_case \
  --max_seq_length=128 \
  --optimizer=adamw \
  --task_name=MNLI \
  --warmup_step=1000 \
  --learning_rate=3e-5 \
  --train_step=10000 \
  --save_checkpoints_steps=100 \
  --train_batch_size=128
```

å¯ä»¥åœ¨`run_glue.sh` ä¸­æ‰¾åˆ°æ¯ä¸ªGLUEä»»åŠ¡çš„é»˜è®¤æ ‡å¿—å€¼ã€‚ 

æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®ä¾‹å¦‚`--albert_hub_module_handle=https://tfhub.dev/google/albert_base/1` ä½¿ç”¨TF-Hubæ¨¡å—, è€Œæ˜¯åŸå§‹checkpointå¼€å§‹å¾®è°ƒæ¨¡å‹ä»£æ›¿ `--init_checkpoint`.

æ‚¨å¯ä»¥åœ¨taræ–‡ä»¶æˆ–tf-hubæ¨¡å—çš„assetæ–‡ä»¶å¤¹ä¸‹æ‰¾åˆ°spm_model_fileã€‚ æ¨¡å‹æ–‡ä»¶çš„åç§°ä¸ºâ€œ30k-clean.modelâ€ã€‚

è¯„ä¼°åï¼Œè„šæœ¬åº”æŠ¥å‘Šå¦‚ä¸‹è¾“å‡ºï¼š 

```
***** Eval results *****
  global_step = ...
  loss = ...
  masked_lm_accuracy = ...
  masked_lm_loss = ...
  sentence_order_accuracy = ...
  sentence_order_loss = ...
```

Fine-tuning on SQuAD
====================
è¦åœ¨SQuAD v1ä¸Šå¾®è°ƒå’Œè¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·ä½¿ç”¨ 
`run_squad_v1.py` script:

```
pip install -r albert/requirements.txt
python -m albert.run_squad_v1 \
  --albert_config_file=... \
  --output_dir=... \
  --train_file=... \
  --predict_file=... \
  --train_feature_file=... \
  --predict_feature_file=... \
  --predict_feature_left_file=... \
  --init_checkpoint=... \
  --spm_model_file=... \
  --do_lower_case \
  --max_seq_length=384 \
  --doc_stride=128 \
  --max_query_length=64 \
  --do_train=true \
  --do_predict=true \
  --train_batch_size=48 \
  --predict_batch_size=8 \
  --learning_rate=5e-5 \
  --num_train_epochs=2.0 \
  --warmup_proportion=.1 \
  --save_checkpoints_steps=5000 \
  --n_best_size=20 \
  --max_answer_length=30
```

æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®ä¾‹å¦‚
`--albert_hub_module_handle=https://tfhub.dev/google/albert_base/1` 
 ä»TF-Hubæ¨¡å—è€Œä¸æ˜¯`--init_checkpoint`åŸå§‹checkpointå¼€å§‹å¾®è°ƒæ¨¡å‹ 

For SQuAD v2, use the `run_squad_v2.py` script:

```
pip install -r albert/requirements.txt
python -m albert.run_squad_v2 \
  --albert_config_file=... \
  --output_dir=... \
  --train_file=... \
  --predict_file=... \
  --train_feature_file=... \
  --predict_feature_file=... \
  --predict_feature_left_file=... \
  --init_checkpoint=... \
  --spm_model_file=... \
  --do_lower_case \
  --max_seq_length=384 \
  --doc_stride=128 \
  --max_query_length=64 \
  --do_train \
  --do_predict \
  --train_batch_size=48 \
  --predict_batch_size=8 \
  --learning_rate=5e-5 \
  --num_train_epochs=2.0 \
  --warmup_proportion=.1 \
  --save_checkpoints_steps=5000 \
  --n_best_size=20 \
  --max_answer_length=30
```

You can fine-tune the model starting from TF-Hub modules instead of raw checkpoints by setting e.g.
`--albert_hub_module_handle=https://tfhub.dev/google/albert_base/1` instead
of `--init_checkpoint`.

Fine-tuning on RACE
===================
For RACE, use the `run_race.py` script:

```
pip install -r albert/requirements.txt
python -m albert.run_race \
  --albert_config_file=... \
  --output_dir=... \
  --train_file=... \
  --eval_file=... \
  --data_dir=...\
  --init_checkpoint=... \
  --spm_model_file=... \
  --max_seq_length=512 \
  --max_qa_length=128 \
  --do_train \
  --do_eval \
  --train_batch_size=32 \
  --eval_batch_size=8 \
  --learning_rate=1e-5 \
  --train_step=12000 \
  --warmup_step=1000 \
  --save_checkpoints_steps=100
```

You can fine-tune the model starting from TF-Hub modules instead of raw
checkpoints by setting e.g.
`--albert_hub_module_handle=https://tfhub.dev/google/albert_base/1` instead
of `--init_checkpoint`.

SentencePiece
=============
ç”Ÿæˆå¥å­å•è¯è¡¨çš„å‘½ä»¤ï¼š 

```
spm_train \
--input all.txt --model_prefix=30k-clean --vocab_size=30000 --logtostderr
--pad_id=0 --unk_id=1 --eos_id=-1 --bos_id=-1
--control_symbols=[CLS],[SEP],[MASK]
--user_defined_symbols="(,),\",-,.,â€“,Â£,â‚¬"
--shuffle_input_sentence=true --input_sentence_size=10000000
--character_coverage=0.99995 --model_type=unigram
```
